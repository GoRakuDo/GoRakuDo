import { readFileSync, writeFileSync, statSync, existsSync, readdirSync, mkdirSync, unlinkSync } from 'fs';
import { join, dirname } from 'path';
import { fileURLToPath } from 'url';
import { createHash } from 'crypto';

/**
 * åŠ¹ç‡åŒ–ã•ã‚ŒãŸè‡ªå‹•æ—¥ä»˜å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ 
 * ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ã¨ä¸¦åˆ—å‡¦ç†ã‚’å‚™ãˆã‚‹
 */

export interface CacheEntry {
 filePath: string;
 lastProcessed: string;
 fileHash: string;
 publishedDate: string;
}

export interface ProcessResult {
 processed: number;
 skipped: number;
 errors: number;
 duration: number;
}

/**
 * ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—
 */
function calculateFileHash(filePath: string): string {
 try {
  const content = readFileSync(filePath, 'utf-8');
  return createHash('md5').update(content).digest('hex');
 } catch (error) {
  console.warn(`Failed to calculate hash for ${filePath}:`, error);
  return '';
 }
}

/**
 * ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—
 */
function getCacheFilePath(): string {
 const __filename = fileURLToPath(import.meta.url);
 const __dirname = dirname(__filename);
 return join(__dirname, '..', '..', '.cache', 'auto-date-cache.json');
}

/**
 * ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã¿
 */
function loadCache(): Map<string, CacheEntry> {
 const cachePath = getCacheFilePath();

 if (!existsSync(cachePath)) {
  return new Map();
 }

 try {
  const cacheData = readFileSync(cachePath, 'utf-8');
  const cacheObject = JSON.parse(cacheData);
  return new Map(Object.entries(cacheObject));
 } catch (error) {
  console.warn('Failed to load cache, starting fresh:', error);
  return new Map();
 }
}

/**
 * ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜
 */
function saveCache(cache: Map<string, CacheEntry>): void {
 const cachePath = getCacheFilePath();
 const cacheDir = dirname(cachePath);

 // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
 if (!existsSync(cacheDir)) {
  mkdirSync(cacheDir, { recursive: true });
 }

 try {
  const cacheObject = Object.fromEntries(cache);
  writeFileSync(cachePath, JSON.stringify(cacheObject, null, 2), 'utf-8');
 } catch (error) {
  console.warn('Failed to save cache:', error);
 }
}

/**
 * ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆæ—¥æ™‚ã‚’å–å¾—
 */
function getFileCreationDate(filePath: string): string {
 try {
  const stats = statSync(filePath);
  return stats.birthtime.toISOString();
 } catch (error) {
  console.warn(`Failed to get creation date for ${filePath}:`, error);
  return new Date().toISOString();
 }
}

/**
 * å˜ä¸€ã®MDXãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
 */
function processMdxFile(filePath: string, cache: Map<string, CacheEntry>): boolean {
 try {
  const currentHash = calculateFileHash(filePath);
  const cacheKey = filePath;
  const cachedEntry = cache.get(cacheKey);

  // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«å­˜åœ¨ã—ã€ãƒãƒƒã‚·ãƒ¥ãŒåŒã˜å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
  if (cachedEntry && cachedEntry.fileHash === currentHash) {
   return false; // ã‚¹ã‚­ãƒƒãƒ—
  }

  const content = readFileSync(filePath, 'utf-8');
  const autoDateRegex = /publishedDate:\s*['"]auto['"]/g;

  if (autoDateRegex.test(content)) {
   const creationDate = getFileCreationDate(filePath);
   const updatedContent = content.replace(
    autoDateRegex,
    `publishedDate: '${creationDate}'`
   );

   writeFileSync(filePath, updatedContent, 'utf-8');

   // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–°
   cache.set(cacheKey, {
    filePath,
    lastProcessed: new Date().toISOString(),
    fileHash: currentHash,
    publishedDate: creationDate,
   });

   console.log(`âœ… Updated publishedDate for ${filePath} to ${creationDate}`);
   return true; // å‡¦ç†æ¸ˆã¿
  }

  return false; // å‡¦ç†ä¸è¦
 } catch (error) {
  console.error(`âŒ Error processing ${filePath}:`, error);
  return false;
 }
}

/**
 * ä¸¦åˆ—å‡¦ç†ã§MDXãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
 */
async function processMdxFilesParallel(filePaths: string[], cache: Map<string, CacheEntry>): Promise<ProcessResult> {
 const startTime = Date.now();
 let processed = 0;
 let skipped = 0;
 let errors = 0;

 // ä¸¦åˆ—å‡¦ç†ã®ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆåŒæ™‚ã«å‡¦ç†ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«æ•°ï¼‰
 const BATCH_SIZE = 10;

 for (let i = 0; i < filePaths.length; i += BATCH_SIZE) {
  const batch = filePaths.slice(i, i + BATCH_SIZE);

  const batchPromises = batch.map(async (filePath) => {
   try {
    const wasProcessed = processMdxFile(filePath, cache);
    if (wasProcessed) {
     processed++;
    } else {
     skipped++;
    }
   } catch (error) {
    console.error(`Error in batch processing ${filePath}:`, error);
    errors++;
   }
  });

  await Promise.all(batchPromises);
 }

 const duration = Date.now() - startTime;

 return {
  processed,
  skipped,
  errors,
  duration,
 };
}

/**
 * åŠ¹ç‡åŒ–ã•ã‚ŒãŸMDXãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
 */
export async function processMdxFilesEfficiently(): Promise<ProcessResult> {
 console.log('+ Starting efficient auto date processing...');

 const cache = loadCache();
 const contentDir = join(process.cwd(), 'src', 'content');
 const mdxFiles: string[] = [];

 // MDXãƒ•ã‚¡ã‚¤ãƒ«ã‚’åé›†
 function collectMdxFiles(directory: string): void {
  try {
   const items = readdirSync(directory);

   for (const item of items) {
    const itemPath = join(directory, item);
    const stats = statSync(itemPath);

    if (stats.isDirectory()) {
     collectMdxFiles(itemPath);
    } else if (item.endsWith('.mdx')) {
     mdxFiles.push(itemPath);
    }
   }
  } catch (error) {
   console.error(`Error collecting files from ${directory}:`, error);
  }
 }

 collectMdxFiles(contentDir);
 console.log(`+ Found ${mdxFiles.length} MDX files`);

 // ä¸¦åˆ—å‡¦ç†ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
 const result = await processMdxFilesParallel(mdxFiles, cache);

 // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜
 saveCache(cache);

 console.log(`+ Processing completed:`);
 console.log(`   - Processed: ${result.processed} files`);
 console.log(`   - Skipped: ${result.skipped} files`);
 console.log(`   - Errors: ${result.errors} files`);
 console.log(`   - Duration: ${result.duration}ms`);

 return result;
}

/**
 * ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
 */
export function clearCache(): void {
 const cachePath = getCacheFilePath();
 if (existsSync(cachePath)) {
  unlinkSync(cachePath);
  console.log('ğŸ—‘ï¸ Cache cleared');
 }
}
